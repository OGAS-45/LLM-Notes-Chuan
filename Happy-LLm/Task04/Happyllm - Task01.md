---
文档名-title: Happyllm - Task01
创建时间-create time: 2025-07-14 22:59
更新时间-modefived time: 2025-07-14 22:59 星期一
文档粗分-text: 笔记
笔记细分-text: 
笔记索引-link: '[[笔记总索引]]'
继承自-link: 
tags:
  - 笔记
模板自: -笔记-规范（2024.6.8）
---

## 前言

理清关系的三句话

LLM 其实是 NLP 领域经典研究方法预训练语言模型（Pretrain Language Model，PLM）的一种衍生成果。

NLP 领域聚焦于人类书写的自然语言文本的处理、理解和生成
- 能力：以注意力机制为模型架构，通过预训练-微调的阶段思想通过在海量无监督文本上进行自监督预训练，实现了强大的自然语言理解能力。
- 问题：统的 PLM 仍然依赖于一定量有监督数据进行下游任务微调，且在自然语言生成任务上性能还不尽如人意，NLP 系统的性能距离人们所期待的通用人工智能还有不小的差距。

LLM 是在 PLM 的基础上，通过大量扩大模型参数、预训练数据规模，并引入指令微调、人类反馈强化学习等手段实现的突破性成果
- 能力：LLM 具备涌现能力，具有强大的上下文学习能力、指令理解能力和文本生成能力。NLP 研究者可以一定程度抛弃大量的监督数据标注工作，通过提供少量监督示例，LLM 即能在指定下游任务上达到媲美大规模微调 PLM 的性能。同时，强大的指令理解能力与文本生成能力使 LLM 能够直接、高效、准确地响应用户指令，从而真正向通用人工智能的目标逼近。

看未来：可以肯定的是，在并不遥远的未来，LLM 及以 LLM 为基础的应用一定会成为人们生活中的基础设施，与每个人的生活、学习、工作密不可分。


## 作者体系

- self-llm（开源大模型食用指南：[https://github.com/datawhalechina/self-llm](https://github.com/datawhalechina/self-llm) ）：为开发者提供一站式开源 LLM 部署、推理、微调的使用教程
- llm-universe（动手学大模型应用开发：[https://github.com/datawhalechina/llm-universe](https://github.com/datawhalechina/llm-universe) ）：指导开发者从零开始搭建自己的 LLM 应用
- happyllm：从 NLP 的基本研究方法出发，根据 LLM 的思路及原理逐层深入，依次为读者剖析 LLM 的架构基础和训练过程。同时，我们会结合目前 LLM 领域最主流的代码框架，演练如何亲手搭建、训练一个 LLM，期以实现授之以鱼，更授之以渔。

## 写给读者的建议

- 基础知识与实战应用
- 第1章～第4章是基础知识部分，从浅入深介绍 LLM 的基本原理
- 第5章～第7章是实战应用部分，将逐步带领读者深入 LLM 的底层细节。

在阅读本书的过程中，建议你将理论和实际相结合。LLM 是一个快速发展、注重实践的领域，我们建议你多投入实战，复现本书提供的各种代码，同时积极参加 LLM 相关的项目与比赛，真正投入到 LLM 开发的浪潮中。

欢迎每一位读者在阅读完本书后加入到 LLM 开发者的行列。
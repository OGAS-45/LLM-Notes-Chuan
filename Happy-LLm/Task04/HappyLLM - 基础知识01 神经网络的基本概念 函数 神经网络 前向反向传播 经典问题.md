---
文档名-title: HappyLLM - 基础知识01 神经网络的基本概念
创建时间-create time: 2025-07-20 23:12
更新时间-modefived time: 2025-07-20 23:12 星期日
文档粗分-text: 笔记
笔记细分-text: 
笔记索引-link: '[[笔记总索引]]'
继承自-link: 
tags:
  - 笔记
模板自: -笔记-规范（2024.6.8）
---

##  函数 F(x)规则
可以表述世界上所有逻辑和知识

Functions describe the world


## 发展历史

最开始认为所有一切都可以用明确的f(x)描述 —— 白盒 —— 符号主义

发现可以靠猜逐步逼近，并且很少参数就行 —— 黑盒 —— 联接主义（叫联接是因为像大脑的生理结构一样具有复杂性）
![[Pasted image 20250720232533.png]]

## 函数到神经网络

### 函数
线性函数，但是无法模拟曲线
$$
f(x) = wx + b
$$

### 加激活函数
加一层变成非线性函数，可以模拟一层曲线
比如平方、sin、cos

![[Pasted image 20250720232503.png]]

$$
f(x) = g(wx + b)
$$

### 增加层数
于是就可以不断迭代层数直至模拟几乎所有非线性曲线

### 增加平行输入数
模拟多个输入，多维度的曲线也可以

$$
f(x1，x2) = g(w1x1 + w2x2 + b)
$$

抽象成输入和输出

![[Pasted image 20250720232328.png]]

### 前向传播
计算数值逐步往前传递
![[Pasted image 20250720232546.png]]

###  总结

神经网络本质就是线性函数套上一个激活函数不断组合成一个非常复杂的非线性函数。目的就是计算w与b使其可以拟合真实数据

## 如何计算

### 终极问题：拟合得好才是真的好

通过预测值和真实值的相差计算损失函数，收获均方误差

![[Pasted image 20250720233640.png]]

### 简单函数 - 偏导为0

因为x和y都是已经给定的值，所以找到均方误差最小的点就好

即寻找一个关于w和b的函数的最低点，即这个点对w和b的偏导数都为0的点

![[Pasted image 20250720233837.png]]

![[Pasted image 20250720233840.png]]

![[Pasted image 20250720233847.png]]

### 复杂函数 - 无法求导，猜，一点点试

复杂函数中针对太多元素了，于是改变方式，开始直接猜

### 偏导数
通过修改一点点的w，了解到最后损失移动的方向
![[Pasted image 20250720235741.png]]

### 学习率
于是就知道了要往反方向去一点点调整，每次调整的步数就叫做学习率
![[Pasted image 20250720235743.png]]


### 梯度
每次变化的方向和步长结合的向量就叫做梯度
![[Pasted image 20250720235826.png]]
![[Pasted image 20250720235917.png]]

## 末端变化
因为调整wb的参数对最后一层的影响最为直观，所以先从最后的参数调整开始，
![[Pasted image 20250721000043.png]]
![[Pasted image 20250721000144.png]]

### 链式法则
每次调整后就得到次一层的调整方向，逐步演进
![[Pasted image 20250721000235.png]]
![[Pasted image 20250721000055.png]]
![[Pasted image 20250721000216.png]]


### 反向传播
逐步往前传递，从右向左一次传导，逐步更新每一层的参数
![[Pasted image 20250721000243.png]]

## 总结

神经网络本质就是线性函数套上一个激活函数不断组合成一个非常复杂的非线性函数。并且巧妙地通过梯度下降，一点一点计算出神经网络中的一组合适的参数。目的就是计算w与b使其可以拟合真实数据

## 解决神经网络中涌现的问题：

### 经典问题：过拟合
神经网络不是无脑足够大就可以的，训练数据上很完美，真实数据上很糟糕

![[Pasted image 20250721001407.png]]

计算进入了噪声和随机波动

### 解决过拟合的其他方式：

#### 模型入手
简化模型复杂度（减少参数和层数）

#### 数据入手
增加训练数据的量
数据增强
以增加鲁棒性

#### 训练过程入手
提前终止训练过程

##### dropout
训练时每次都随机丢弃一部分参数

##### 正则化 ：损失函数加上有正则化系数相乘的惩罚项，并且叫做超参数
将损失函数修改成会将原本的参数值加入进去考虑，如果修改过多大于应该有的值，则可以认为是不合适的
![[Pasted image 20250721001908.png]]
![[Pasted image 20250721001902.png]]
![[Pasted image 20250721001854.png]]

https://www.bilibili.com/video/BV1RqXRYDEe2 6min处
### 其他遇到的问题

梯度消失
梯度爆炸
收敛速度
计算开销

### 其他问题的解法

梯度裁剪
残差网络
权重初始化
归一化
动量法 RMSRrop Adam
mini-batch

### 之后会讲

卷积网络CNN - 通过卷积层池化层处理图片数据
循环网络RNN - 利用上下文处理序列数据